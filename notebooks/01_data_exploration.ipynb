{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed RCT - Data Exploration\n",
    "\n",
    "This notebook explores the PubMed 20k RCT dataset for sentence classification in medical abstracts.\n",
    "\n",
    "**Goal:** Understand data structure, class distribution, and sentence length statistics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "The PubMed RCT dataset contains medical abstracts where each sentence is labeled with one of 5 categories:\n",
    "- BACKGROUND, OBJECTIVE, METHODS, RESULTS, CONCLUSIONS\n",
    "\n",
    "File format:\n",
    "```\n",
    "###ABSTRACT_ID\n",
    "LABEL\\tSENTENCE_TEXT\n",
    "...\n",
    "(blank line between abstracts)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_pubmed_data(filepath):\n",
    "    \"\"\"Load and preprocess PubMed RCT data from a text file.\n",
    "    Returns a list of dicts with keys: target, text, line_number, total_lines.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    samples = []\n",
    "    abstract_lines = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_lines = \"\"\n",
    "        elif line.isspace():\n",
    "            split = abstract_lines.splitlines()\n",
    "            for i, al in enumerate(split):\n",
    "                parts = al.split(\"\\t\")\n",
    "                if len(parts) == 2:\n",
    "                    samples.append({\n",
    "                        \"target\": parts[0],\n",
    "                        \"text\": parts[1].lower(),\n",
    "                        \"line_number\": i,\n",
    "                        \"total_lines\": len(split) - 1\n",
    "                    })\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "\n",
    "    return samples"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Path to the dataset\n",
    "DATA_DIR = \"../data/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "\n",
    "# Load all splits\n",
    "train_samples = load_pubmed_data(os.path.join(DATA_DIR, \"train.txt\"))\n",
    "val_samples = load_pubmed_data(os.path.join(DATA_DIR, \"dev.txt\"))\n",
    "test_samples = load_pubmed_data(os.path.join(DATA_DIR, \"test.txt\"))\n",
    "\n",
    "print(f\"Train: {len(train_samples):,} samples\")\n",
    "print(f\"Val:   {len(val_samples):,} samples\")\n",
    "print(f\"Test:  {len(test_samples):,} samples\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Convert to DataFrames\n",
    "train_df = pd.DataFrame(train_samples)\n",
    "val_df = pd.DataFrame(val_samples)\n",
    "test_df = pd.DataFrame(test_samples)\n",
    "\n",
    "train_df.head(10)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution\n",
    "\n",
    "Let us check how the classes are distributed in the training set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "class_counts = train_df[\"target\"].value_counts()\n",
    "\n",
    "print(\"Class distribution (training set):\")\n",
    "print(class_counts)\n",
    "print()\n",
    "print(\"Percentages:\")\n",
    "print((class_counts / len(train_df) * 100).round(2))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(class_counts.index, class_counts.values, color=\"steelblue\", edgecolor=\"black\")\n",
    "\n",
    "for i, (label, val) in enumerate(class_counts.items()):\n",
    "    pct = val / len(train_df) * 100\n",
    "    ax.text(i, val + 50, f\"{val:,}\\n({pct:.1f}%)\", ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "ax.set_xlabel(\"Label\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "ax.set_title(\"Class Distribution - Training Set\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "ratio = class_counts.max() / class_counts.min()\n",
    "print(f\"Imbalance ratio (max/min): {ratio:.2f}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence Length Analysis\n",
    "\n",
    "We analyze the number of words per sentence to decide on `max_length` for padding in deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "sent_lengths = [len(s.split()) for s in train_df[\"text\"]]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.hist(sent_lengths, bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
    "ax1.axvline(np.mean(sent_lengths), color=\"red\", linestyle=\"--\", label=f\"Mean: {np.mean(sent_lengths):.1f}\")\n",
    "ax1.axvline(np.median(sent_lengths), color=\"green\", linestyle=\"--\", label=f\"Median: {np.median(sent_lengths):.0f}\")\n",
    "ax1.set_xlabel(\"Number of words\")\n",
    "ax1.set_ylabel(\"Frequency\")\n",
    "ax1.set_title(\"Sentence Length Distribution\")\n",
    "ax1.legend()\n",
    "\n",
    "bp = ax2.boxplot(sent_lengths, vert=True, patch_artist=True)\n",
    "bp[\"boxes\"][0].set_facecolor(\"lightblue\")\n",
    "ax2.set_ylabel(\"Number of words\")\n",
    "ax2.set_title(\"Box Plot\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "print(\"Sentence length statistics:\")\n",
    "print(f\"  Mean:            {np.mean(sent_lengths):.1f}\")\n",
    "print(f\"  Median:          {np.median(sent_lengths):.0f}\")\n",
    "print(f\"  Std:             {np.std(sent_lengths):.1f}\")\n",
    "print(f\"  Min:             {np.min(sent_lengths)}\")\n",
    "print(f\"  Max:             {np.max(sent_lengths)}\")\n",
    "print(f\"  95th percentile: {np.percentile(sent_lengths, 95):.0f}\")\n",
    "print(f\"  99th percentile: {np.percentile(sent_lengths, 99):.0f}\")\n",
    "print()\n",
    "p95 = int(np.percentile(sent_lengths, 95))\n",
    "print(f\"Recommended max_length = {p95} (covers 95% of sentences)\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}