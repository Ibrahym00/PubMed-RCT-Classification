{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PubMed RCT - Embeddings Model (GloVe)\n",
    "\n",
    "This notebook builds a classification model using pre-trained GloVe word embeddings.\n",
    "\n",
    "**Architecture:**\n",
    "- TextVectorization layer\n",
    "- Pre-trained GloVe embeddings (100d, frozen)\n",
    "- GlobalAveragePooling1D\n",
    "- Dense(64, ReLU) + Dropout(0.5)\n",
    "- Dense(5, softmax)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "%matplotlib inline"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "def load_pubmed_data(filepath):\n",
    "    \"\"\"Load and preprocess PubMed RCT data from a text file.\n",
    "    Returns a list of dicts with keys: target, text, line_number, total_lines.\n",
    "    \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    samples = []\n",
    "    abstract_lines = \"\"\n",
    "\n",
    "    for line in lines:\n",
    "        if line.startswith(\"###\"):\n",
    "            abstract_lines = \"\"\n",
    "        elif line.isspace():\n",
    "            split = abstract_lines.splitlines()\n",
    "            for i, al in enumerate(split):\n",
    "                parts = al.split(\"\\t\")\n",
    "                if len(parts) == 2:\n",
    "                    samples.append({\n",
    "                        \"target\": parts[0],\n",
    "                        \"text\": parts[1].lower(),\n",
    "                        \"line_number\": i,\n",
    "                        \"total_lines\": len(split) - 1\n",
    "                    })\n",
    "        else:\n",
    "            abstract_lines += line\n",
    "\n",
    "    return samples"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "DATA_DIR = \"../data/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\"\n",
    "CLASS_NAMES = [\"BACKGROUND\", \"OBJECTIVE\", \"METHODS\", \"RESULTS\", \"CONCLUSIONS\"]\n",
    "MAX_LENGTH = 55\n",
    "\n",
    "train_df = pd.DataFrame(load_pubmed_data(os.path.join(DATA_DIR, \"train.txt\")))\n",
    "val_df = pd.DataFrame(load_pubmed_data(os.path.join(DATA_DIR, \"dev.txt\")))\n",
    "test_df = pd.DataFrame(load_pubmed_data(os.path.join(DATA_DIR, \"test.txt\")))\n",
    "\n",
    "train_sentences = train_df[\"text\"].to_numpy()\n",
    "val_sentences = val_df[\"text\"].to_numpy()\n",
    "test_sentences = test_df[\"text\"].to_numpy()\n",
    "\n",
    "# One-hot encode labels\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "train_labels = encoder.fit_transform(train_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "val_labels = encoder.transform(val_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "test_labels = encoder.transform(test_df[\"target\"].to_numpy().reshape(-1, 1))\n",
    "\n",
    "print(f\"Train: {len(train_sentences)} | Val: {len(val_sentences)} | Test: {len(test_sentences)}\")\n",
    "print(f\"Classes: {encoder.categories_[0]}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "max_tokens = 68000\n",
    "\n",
    "text_vectorizer = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_sequence_length=MAX_LENGTH\n",
    ")\n",
    "text_vectorizer.adapt(train_sentences)\n",
    "\n",
    "vocab = text_vectorizer.get_vocabulary()\n",
    "print(f\"Vocabulary size: {len(vocab)}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load GloVe Embeddings\n",
    "\n",
    "Download GloVe from https://nlp.stanford.edu/data/glove.6B.zip and place `glove.6B.100d.txt` in the `data/` folder."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "embedding_dim = 100\n",
    "glove_path = \"../data/glove.6B.100d.txt\"\n",
    "\n",
    "# Load GloVe vectors into a dictionary\n",
    "embeddings_index = {}\n",
    "if os.path.exists(glove_path):\n",
    "    with open(glove_path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = vector\n",
    "    print(f\"Loaded {len(embeddings_index)} word vectors from GloVe\")\n",
    "else:\n",
    "    print(f\"GloVe file not found at {glove_path}. Using random embeddings.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Build embedding matrix matching our vocabulary\n",
    "embedding_matrix = np.zeros((len(vocab), embedding_dim))\n",
    "found = 0\n",
    "\n",
    "for i, word in enumerate(vocab):\n",
    "    vec = embeddings_index.get(word)\n",
    "    if vec is not None:\n",
    "        embedding_matrix[i] = vec\n",
    "        found += 1\n",
    "\n",
    "print(f\"Words found in GloVe: {found}/{len(vocab)} ({found/len(vocab)*100:.1f}%)\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "inputs = layers.Input(shape=[], dtype=\"string\")\n",
    "\n",
    "x = text_vectorizer(inputs)\n",
    "x = layers.Embedding(\n",
    "    input_dim=len(vocab),\n",
    "    output_dim=embedding_dim,\n",
    "    embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False  # freeze pre-trained embeddings\n",
    ")(x)\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(len(CLASS_NAMES), activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(inputs, outputs, name=\"glove_model\")\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "history = model.fit(\n",
    "    train_sentences, train_labels,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(val_sentences, val_labels)\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.plot(history.history[\"loss\"], label=\"Train\")\n",
    "ax1.plot(history.history[\"val_loss\"], label=\"Validation\")\n",
    "ax1.set_title(\"Loss\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.plot(history.history[\"accuracy\"], label=\"Train\")\n",
    "ax2.plot(history.history[\"val_accuracy\"], label=\"Validation\")\n",
    "ax2.set_title(\"Accuracy\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "test_loss, test_acc = model.evaluate(test_sentences, test_labels, verbose=0)\n",
    "print(f\"Test loss: {test_loss:.4f}\")\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "preds = np.argmax(model.predict(test_sentences, verbose=0), axis=1)\n",
    "true = np.argmax(test_labels, axis=1)\n",
    "\n",
    "print()\n",
    "print(classification_report(true, preds, target_names=CLASS_NAMES))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(true, preds)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "plt.title(\"Confusion Matrix - Embeddings Model\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "os.makedirs(\"../results\", exist_ok=True)\n",
    "\n",
    "results = {\n",
    "    \"model_name\": \"Embeddings Model (GloVe)\",\n",
    "    \"test_accuracy\": float(test_acc),\n",
    "    \"test_loss\": float(test_loss),\n",
    "}\n",
    "with open(\"../results/embeddings_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"Results saved. Test accuracy = {test_acc*100:.2f}%\")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ]
}